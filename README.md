# Silent Drift Detection

Lightweight pipeline for detecting "silent drift" in ML telemetry using
multivariate statistical anomaly detection (Mahalanobis distance).

This repository provides an end-to-end demonstration: synthetic data
generation with injected drift, rolling-window feature engineering, a
Mahalanobis-based detector, evaluation helpers, and an exploration
notebook for analysis and visualizations.

## Repository Layout

- `main.py` - End-to-end orchestrator that saves artifacts to `data/`.
- `src/` - Core modules:
  - `data_generator.py` - Synthetic telemetry generator with injected drift.
  - `preprocessing.py` - Rolling-window feature extraction (mean/std).
  - `anomaly_model.py` - `DriftDetector` implementation (Mahalanobis distance).
  - `evaluator.py` - Small helpers to summarize anomaly counts and ratios.
- `data/raw/` - Raw telemetry CSVs (generated by `main.py`).
- `data/processed/` - Feature CSVs and saved plots (e.g. `anomaly_scores.png`).
- `notebooks/exploration.ipynb` - Jupyter notebook for interactive analysis.
- `tests/` - Unit tests (smoke tests for the detector).

## Quick Start (macOS / zsh)

1. Create and activate a virtual environment (recommended):

```bash
python3 -m venv .venv
source .venv/bin/activate
```

2. Install dependencies:

```bash
pip install -r requirements.txt
# or install manually
pip install numpy pandas scikit-learn matplotlib seaborn pytest
```

3. Run the full pipeline:

```bash
python main.py
```

This will generate synthetic telemetry, compute rolling features, fit the
detector on the clean baseline period, score the monitoring period, save
`data/processed/anomaly_scores.png`, and print evaluation metrics.

4. Open the exploration notebook (interactive analysis):

```bash
jupyter notebook notebooks/exploration.ipynb
```

## Notes & Best Practices

- Baseline vs Monitoring: The pipeline uses a baseline split (default 30%)
  to learn the reference distribution. Always derive detection thresholds
  from the clean baseline; deriving them from the monitoring data is an
  anti-pattern (it biases thresholds upward and masks drift).
- Threshold choice: Default is the 95th percentile of baseline Mahalanobis
  scores (recommended). If labeled data exist, use ROC/AUC to optimize.
- Headless operation: Plots are saved to `data/processed/` for CI-friendly runs.

## Running Tests

Run the test suite with:

```bash
pytest -q
```

## Contributing

- Add feature engineering under `src/preprocessing.py`.
- Try different detectors in `src/anomaly_model.py` and compare results.

## License

Provided as an educational reference. Use or adapt as needed.

## Contact

Open an issue or contact the maintainer for help integrating this into
production telemetry systems.
# Silent Drift Detection

Simple end-to-end anomaly detection pipeline for telemetry data.

Quickstart

1. Create a Python environment (recommended):

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

2. Run the pipeline (generate, preprocess, train, evaluate):

```bash
python main.py --n 1000 --seed 42 --train --eval --output-dir ./output
```

3. Run tests:

```bash
pytest -q
```

Project layout

- `src/data_generator.py`: synthetic telemetry generator
- `src/preprocessing.py`: cleaning and aggregation
- `src/anomaly_model.py`: simple IsolationForest wrapper
- `src/evaluator.py`: precision/recall/F1 metrics
- `main.py`: CLI orchestrator
- `tests/test_anomaly.py`: small end-to-end unit test

Extending

- Replace the model in `src/anomaly_model.py` to try different algorithms.
- Add more feature engineering in `src/preprocessing.py`.


